{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Sarcastic Headlines Using Sklearn's Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this project, we wanted to be able to read a news headline and use logistic regression to predict if it was a genuine, non-satirical news article or a humorous, satarical news article. \n",
    "\n",
    "In essence, we are using logistic regression to make a binary classification for news headlines as \"sarcastic\" or \"non-sarcastic\"\n",
    "\n",
    "Our dataset comes from https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection. It is filled news articles from both The Huffington Post and The Onion, as well as labels for sarcastic and non-sarcastic for each news article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we cleaned our data-file to get rid of any articles that used unicode characters like accented vowels because they throw off our vectorizer later.\n",
    "\n",
    "Here, we open our data-file using the python's csv.reader() function and load each tuple of data into two separate lists.\n",
    "\n",
    "'data' holds raw text strings of our news articles, and 'data_labels' holds 1's (for sarcastic headlines) and 0's (for non-sarcastic headlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "data_labels = []\n",
    "\n",
    "with open(\"./sarcasm_oneliners/headlines.csv\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    \n",
    "    for row in csvreader:\n",
    "        data.append(row[1])\n",
    "        label = row[0]\n",
    "\n",
    "        if label == '0':\n",
    "            data_labels.append('non-sarcastic')\n",
    "        else:\n",
    "            data_labels.append('sarcastic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to vectorize each headline to extract our important features from the text. We do this by using sklearn's built in feature extractor for text.\n",
    "\n",
    "We feed our data into the word count vectorizer in order to build a list of words and their frequencies.\n",
    "\n",
    "We then save this into an n dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', lowercase = False)\n",
    "\n",
    "features = vectorizer.fit_transform(data)\n",
    "\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use 'sklearn.model_selection.train_test_split' to partition our data set into a test set and a training set.\n",
    "\n",
    "train_test_split returns 4 lists\n",
    "- X_train: our 80% portion of 'features_nd' used to train our logistic regression model\n",
    "- X_test: our 20% portion of 'features_nd' which our model will try to label as 'sarcastic' or 'non-sarcastic'\n",
    "- y_train: our 80% portion of 'data_labels' used to map every data point in X_train to it's real label\n",
    "- y_test: our 20% portion of 'data_labels' that are the true values of the labels assigned to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_nd,\n",
    "    data_labels,\n",
    "    test_size=0.20,\n",
    "    train_size=0.80,\n",
    "    random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import our logistic regression model. \n",
    "\n",
    "If \"solver='lbfgs'\" wasn't specified, we'd get a warning that no solver was specified for the model and that 'lbfgs' would be used by default. \n",
    "\n",
    "(We're not too sure what that does to be honest, but we decided putting it on the LogisticRegression object so that it doesn't yell at us with a warning was better than seeing a red warning everytime we ran the jupyter notebook cell lol.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model on our training set of data specified by train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it to predict the labels for our test set.\n",
    "\n",
    "Then we save two more lists\n",
    "- y_pred: our predicted labels that our model gave to each headline in y_pred\n",
    "- y_prob: the probabilities assigned to each label for the given headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcastic\n",
      "[0.4611363 0.5388637]\n",
      "non-sarcastic\n",
      "[0.57036373 0.42963627]\n",
      "sarcastic\n",
      "[0.01015833 0.98984167]\n",
      "sarcastic\n",
      "[0.28094057 0.71905943]\n",
      "non-sarcastic\n",
      "[0.66878904 0.33121096]\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "y_prob = log_model.predict_proba(X_test)\n",
    "\n",
    "for i in range(29,34):\n",
    "    print(y_pred[i])\n",
    "    print(y_prob[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can play around with seeing the different labels that we assigned to our news articles in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sarcastic - (80.73% probability)\n",
      "6-year-old cries when told mtm productions kitten dead by now\n",
      "\n",
      "sarcastic - (72.10% probability)\n",
      "employees on other end of conference call just want it to be over\n",
      "\n",
      "non-sarcastic - (51.88% probability)\n",
      "miss france iris mittenaere wins miss universe crown\n",
      "\n",
      "sarcastic - (84.28% probability)\n",
      "ex-iranian president mahmoud ahmadinejad plans to run again\n",
      "\n",
      "sarcastic - (86.23% probability)\n",
      "glitch in country allows citizens to temporarily walk through tables\n",
      "\n",
      "non-sarcastic - (57.33% probability)\n",
      "12 stunning photos of 'tiny dancers' caught in action\n",
      "\n",
      "sarcastic - (74.14% probability)\n",
      "david byrne holds up old suit to show how far he's come in weight loss journey\n",
      "\n",
      "non-sarcastic - (73.89% probability)\n",
      "nordstrom just low-key dropped a huge fall sale\n",
      "\n",
      "non-sarcastic - (96.74% probability)\n",
      "pope francis pardons those who dodged the draft during crusades\n",
      "\n",
      "non-sarcastic - (96.69% probability)\n",
      "here's what actually happens to the money in wishing wells\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "j = random.randint(0,len(X_test)-10)\n",
    "for i in range(j,j+10):\n",
    "    prob_i = y_prob[i]\n",
    "    prob = 0\n",
    "    \n",
    "    if prob_i[0] < prob_i[1]:\n",
    "        prob = prob_i[1]\n",
    "    else:\n",
    "        prob = prob_i[0]\n",
    "        \n",
    "    print(y_pred[i] + \" - (\" + '%.2f' % (prob*100) + \"% probability)\")\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(data[ind].strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the predicted labels is fun, but now let's see how accurate our model is.\n",
    "\n",
    "We'll use sklearn's accuracy_score function to see how on the mark our model is with it's predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78% isn't terrible, but definitely can be improved later on. Using a previous data set of just sarcastic and non-sarcastic Reddit comments, our model had but a 50% accuracy (which is no better than guessing). ~80% is not bad at all!\n",
    "\n",
    "For our purposes, we also use sklearn's classification_report to see our precision, recall, and f-1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-sarcastic       0.76      0.78      0.77       370\n",
      "    sarcastic       0.81      0.79      0.80       430\n",
      "\n",
      "    micro avg       0.79      0.79      0.79       800\n",
      "    macro avg       0.79      0.79      0.79       800\n",
      " weighted avg       0.79      0.79      0.79       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sarcastic', 'sarcastic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add any headline to the dataset, re-vectorize it and extract numerical features, fit our model, and predict our newly added headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append('Pathetic: Mom And Dad Are Trying To Pass Off The Trip To Grandma\\'s House As This Year\\'s Family Vacation')\n",
    "\n",
    "features = vectorizer.fit_transform(data)\n",
    "\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features_nd[:len(features_nd) - 1]\n",
    "X_test = features_nd[-1:]\n",
    "\n",
    "log_model = log_model.fit(X=X_train,y=data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = log_model.predict(X_test)\n",
    "yprob = log_model.predict_proba(X_test)\n",
    "\n",
    "if yprob[0][0] < yprob[0][1]:\n",
    "    yprob = yprob[0][1]\n",
    "else:\n",
    "    yprob = yprob[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-sarcastic - (53.94% probability)\n",
      "Pathetic: Mom And Dad Are Trying To Pass Off The Trip To Grandma's House As This Year's Family Vacation\n"
     ]
    }
   ],
   "source": [
    "out = ypred[0] + \" - (\" + ('%.2f' % (yprob*100)) + \"% probability)\"\n",
    "\n",
    "print(out)\n",
    "ind = features_nd.tolist().index(X_test[0].tolist())\n",
    "print(data[ind].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
