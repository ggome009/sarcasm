{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Sarcastic Headlines Using Sklearn's Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this project, we wanted to be able to read a news headline and use logistic regression to predict if it was a genuine, non-satirical news article or a humorous, satarical news article. \n",
    "\n",
    "In essence, we are using logistic regression to make a binary classification for news headlines as \"sarcastic\" or \"non-sarcastic\"\n",
    "\n",
    "Our dataset comes from https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection. It is filled news articles from both The Huffington Post and The Onion, as well as labels for sarcastic and non-sarcastic for each news article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we cleaned our data-file to get rid of any articles that used unicode characters like accented vowels because they throw off our vectorizer later.\n",
    "\n",
    "Here, we open our data-file using the python's csv.reader() function and load each tuple of data into two separate lists.\n",
    "\n",
    "'data' holds raw text strings of our news articles, and 'data_labels' holds 1's (for sarcastic headlines) and 0's (for non-sarcastic headlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "data_labels = []\n",
    "\n",
    "with open(\"./sarcasm_oneliners/headlines.csv\") as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    \n",
    "    for row in csvreader:\n",
    "        data.append(row[1])\n",
    "        label = row[0]\n",
    "\n",
    "        if label == '0':\n",
    "            data_labels.append('non-sarcastic')\n",
    "        else:\n",
    "            data_labels.append('sarcastic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to vectorize each headline to extract our important features from the text. We do this by using sklearn's built in feature extractor for text.\n",
    "\n",
    "We feed our data into the word count vectorizer in order to build a list of words and their frequencies.\n",
    "\n",
    "We then save this into an n dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', lowercase = False)\n",
    "\n",
    "features = vectorizer.fit_transform(data)\n",
    "\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use 'sklearn.model_selection.train_test_split' to partition our data set into a test set and a training set.\n",
    "\n",
    "train_test_split returns 4 lists\n",
    "- X_train: our 80% portion of 'features_nd' used to train our logistic regression model\n",
    "- X_test: our 20% portion of 'features_nd' which our model will try to label as 'sarcastic' or 'non-sarcastic'\n",
    "- y_train: our 80% portion of 'data_labels' used to map every data point in X_train to it's real label\n",
    "- y_test: our 20% portion of 'data_labels' that are the true values of the labels assigned to X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_nd,\n",
    "    data_labels,\n",
    "    test_size=0.20,\n",
    "    train_size=0.80,\n",
    "    random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import our logistic regression model. \n",
    "\n",
    "If \"solver='lbfgs'\" wasn't specified, we'd get a warning that no solver was specified for the model and that 'lbfgs' would be used by default. \n",
    "\n",
    "(We're not too sure what that does to be honest, but we decided putting it on the LogisticRegression object so that it doesn't yell at us with a warning was better than seeing a red warning everytime we ran the jupyter notebook cell lol.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model on our training set of data specified by train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, we can use it to predict the labels for our test set.\n",
    "\n",
    "Then we save two more lists\n",
    "- y_pred: our predicted labels that our model gave to each headline in y_pred\n",
    "- y_prob: the probabilities assigned to each label for the given headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)\n",
    "y_prob = log_model.predict_proba(X_test)\n",
    "\n",
    "for i in range(5):\n",
    "    print(y_pred[i])\n",
    "    print(y_prob[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can play around with seeing the different labels that we assigned to our news articles in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-sarcastic - (70.17% probability)\n",
      "michael b. jordan sets fire to first 'fahrenheit 451' trailer\n",
      "\n",
      "non-sarcastic - (88.49% probability)\n",
      "jessica simpson and eric johnson throw it back to 'national lampoon's vacation' for halloween\n",
      "\n",
      "sarcastic - (97.10% probability)\n",
      "man visiting hometown amazed to find all his childhood insecurities still there\n",
      "\n",
      "sarcastic - (92.19% probability)\n",
      "man knows unsettling amount about nationwide age-of-consent laws\n",
      "\n",
      "non-sarcastic - (52.38% probability)\n",
      "u.s. senators share their #metoo sex harassment stories\n",
      "\n",
      "sarcastic - (73.72% probability)\n",
      "father of war hero near tears as he pleads with paul ryan\n",
      "\n",
      "non-sarcastic - (91.59% probability)\n",
      "jimmy carter contemplating dying right here and now\n",
      "\n",
      "non-sarcastic - (61.67% probability)\n",
      "bill clinton agrees to disclose guacamole recipe\n",
      "\n",
      "non-sarcastic - (98.47% probability)\n",
      "carson says trump knows judge attack was wrong\n",
      "\n",
      "sarcastic - (73.24% probability)\n",
      "who declares sierra leone free of ebola\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "j = random.randint(0,len(X_test)-10)\n",
    "for i in range(j,j+10):\n",
    "    prob_i = y_prob[i]\n",
    "    prob = 0\n",
    "    \n",
    "    if prob_i[0] < prob_i[1]:\n",
    "        prob = prob_i[1]\n",
    "    else:\n",
    "        prob = prob_i[0]\n",
    "        \n",
    "    print(y_pred[i] + \" - (\" + '%.2f' % (prob*100) + \"% probability)\")\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(data[ind].strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the predicted labels is fun, but now let's see how accurate our model is.\n",
    "\n",
    "We'll use sklearn's accuracy_score function to see how on the mark our model is with it's predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78% isn't terrible, but definitely can be improved later on. Using a previous data set of just sarcastic and non-sarcastic Reddit comments, our model had but a 50% accuracy (which is no better than guessing). ~80% is not bad at all!\n",
    "\n",
    "For our purposes, we also use sklearn's classification_report to see our precision, recall, and f-1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "non-sarcastic       0.76      0.78      0.77       370\n",
      "    sarcastic       0.81      0.79      0.80       430\n",
      "\n",
      "    micro avg       0.79      0.79      0.79       800\n",
      "    macro avg       0.79      0.79      0.79       800\n",
      " weighted avg       0.79      0.79      0.79       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sarcastic', 'sarcastic']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
